{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare', 'male',\n",
      "       'age_was_missing', 'embarked_from_cherbourg',\n",
      "       'embarked_from_queenstown', 'embarked_from_southampton'],\n",
      "      dtype='object')\n",
      "(891, 11)\n",
      "         survived      pclass         age       sibsp       parch        fare  \\\n",
      "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208   \n",
      "std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429   \n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400   \n",
      "50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200   \n",
      "75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000   \n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200   \n",
      "\n",
      "             male  embarked_from_cherbourg  embarked_from_queenstown  \\\n",
      "count  891.000000               891.000000                891.000000   \n",
      "mean     0.647587                 0.188552                  0.086420   \n",
      "std      0.477990                 0.391372                  0.281141   \n",
      "min      0.000000                 0.000000                  0.000000   \n",
      "25%      0.000000                 0.000000                  0.000000   \n",
      "50%      1.000000                 0.000000                  0.000000   \n",
      "75%      1.000000                 0.000000                  0.000000   \n",
      "max      1.000000                 1.000000                  1.000000   \n",
      "\n",
      "       embarked_from_southampton  \n",
      "count                 891.000000  \n",
      "mean                    0.722783  \n",
      "std                     0.447876  \n",
      "min                     0.000000  \n",
      "25%                     0.000000  \n",
      "50%                     1.000000  \n",
      "75%                     1.000000  \n",
      "max                     1.000000  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/titanic_all_numeric.csv')\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 10)\n"
     ]
    }
   ],
   "source": [
    "predictors = df.drop('survived', axis=1).values\n",
    "n_cols = predictors.shape[1]\n",
    "print(predictors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "target = to_categorical(df.survived)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(0.000001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/1\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 6.4625 - acc: 0.3949 - val_loss: 6.5914 - val_acc: 0.3582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6405e78d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, target, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9995969  0.99999976 0.99977106 0.99999845 0.99997497 0.99992347\n",
      " 1.         0.99816823 0.9999523  0.9994086 ]\n"
     ]
    }
   ],
   "source": [
    "pred_prob = predictions[:, 1]\n",
    "print(pred_prob[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/30\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 1.0750 - acc: 0.5827 - val_loss: 1.0535 - val_acc: 0.6493\n",
      "Epoch 2/30\n",
      "623/623 [==============================] - 0s 167us/step - loss: 0.7899 - acc: 0.6260 - val_loss: 0.7138 - val_acc: 0.6866\n",
      "Epoch 3/30\n",
      "623/623 [==============================] - 0s 166us/step - loss: 0.7249 - acc: 0.6517 - val_loss: 0.6473 - val_acc: 0.6978\n",
      "Epoch 4/30\n",
      "623/623 [==============================] - 0s 171us/step - loss: 0.6893 - acc: 0.6677 - val_loss: 0.5521 - val_acc: 0.7090\n",
      "Epoch 5/30\n",
      "623/623 [==============================] - 0s 164us/step - loss: 0.6210 - acc: 0.6998 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 6/30\n",
      "623/623 [==============================] - 0s 170us/step - loss: 0.6154 - acc: 0.6854 - val_loss: 0.5034 - val_acc: 0.7537\n",
      "Epoch 7/30\n",
      "623/623 [==============================] - 0s 324us/step - loss: 0.5702 - acc: 0.7223 - val_loss: 0.5318 - val_acc: 0.7687\n",
      "Epoch 8/30\n",
      "623/623 [==============================] - 0s 401us/step - loss: 0.5871 - acc: 0.7207 - val_loss: 0.6300 - val_acc: 0.6493\n",
      "Epoch 9/30\n",
      "623/623 [==============================] - 0s 525us/step - loss: 0.5428 - acc: 0.7400 - val_loss: 0.5240 - val_acc: 0.7388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x640a6d048>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "model.fit(predictors, target, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/15\n",
      "712/712 [==============================] - 2s 2ms/step - loss: 2.4690 - acc: 0.6152 - val_loss: 1.4324 - val_acc: 0.6480\n",
      "Epoch 2/15\n",
      "712/712 [==============================] - 0s 108us/step - loss: 1.3103 - acc: 0.5927 - val_loss: 1.1174 - val_acc: 0.5866\n",
      "Epoch 3/15\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.9202 - acc: 0.6166 - val_loss: 0.6856 - val_acc: 0.6983\n",
      "Epoch 4/15\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.7363 - acc: 0.6615 - val_loss: 0.6165 - val_acc: 0.7207\n",
      "Epoch 5/15\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6868 - acc: 0.6517 - val_loss: 0.5926 - val_acc: 0.6872\n",
      "Epoch 6/15\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6449 - acc: 0.6531 - val_loss: 0.5545 - val_acc: 0.7151\n",
      "Epoch 7/15\n",
      "712/712 [==============================] - 0s 105us/step - loss: 0.6217 - acc: 0.6699 - val_loss: 0.5406 - val_acc: 0.7598\n",
      "Epoch 8/15\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6086 - acc: 0.6882 - val_loss: 0.5454 - val_acc: 0.7095\n",
      "Epoch 9/15\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6029 - acc: 0.6840 - val_loss: 0.5425 - val_acc: 0.7318\n",
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/15\n",
      "712/712 [==============================] - 2s 2ms/step - loss: 0.7986 - acc: 0.6461 - val_loss: 0.7131 - val_acc: 0.6480\n",
      "Epoch 2/15\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.7349 - acc: 0.6419 - val_loss: 0.5236 - val_acc: 0.7318\n",
      "Epoch 3/15\n",
      "712/712 [==============================] - 0s 176us/step - loss: 0.6146 - acc: 0.6629 - val_loss: 0.5042 - val_acc: 0.7542\n",
      "Epoch 4/15\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.7136 - acc: 0.6629 - val_loss: 0.5032 - val_acc: 0.7765\n",
      "Epoch 5/15\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.5917 - acc: 0.6952 - val_loss: 0.5608 - val_acc: 0.6760\n",
      "Epoch 6/15\n",
      "712/712 [==============================] - 0s 286us/step - loss: 0.7050 - acc: 0.6545 - val_loss: 0.4654 - val_acc: 0.7709\n",
      "Epoch 7/15\n",
      "712/712 [==============================] - 0s 215us/step - loss: 0.6072 - acc: 0.6826 - val_loss: 0.4713 - val_acc: 0.7821\n",
      "Epoch 8/15\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6124 - acc: 0.6994 - val_loss: 0.5058 - val_acc: 0.7709\n"
     ]
    }
   ],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_1.add(Dense(10, activation='relu', input_shape=input_shape))\n",
    "model_1.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4324170867158048, 1.1173877606178795, 0.6855906909071533, 0.6165030533708008, 0.5925991723324333, 0.5544680793192134, 0.5405792723820863, 0.5454323030383893, 0.5425332358429552]\n",
      "[0.7130962149057974, 0.5236370904818594, 0.5041934673013634, 0.5031990327315623, 0.5608293384147089, 0.46537816341362853, 0.4712659033983113, 0.505768342890553]\n"
     ]
    }
   ],
   "source": [
    "print(model_1_training.history['val_loss'])\n",
    "print(model_2_training.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/15\n",
      "712/712 [==============================] - 2s 2ms/step - loss: 2.7136 - acc: 0.4944 - val_loss: 0.6362 - val_acc: 0.7039\n",
      "Epoch 2/15\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6820 - acc: 0.6531 - val_loss: 0.5721 - val_acc: 0.7318\n",
      "Epoch 3/15\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6314 - acc: 0.6615 - val_loss: 0.5184 - val_acc: 0.7598\n",
      "Epoch 4/15\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6358 - acc: 0.6671 - val_loss: 0.5653 - val_acc: 0.6760\n",
      "Epoch 5/15\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6138 - acc: 0.6882 - val_loss: 0.5363 - val_acc: 0.7207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_3 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_3.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "model_3.add(Dense(50, activation='relu'))\n",
    "model_3.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_3.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_2\n",
    "model_3_training = model_3.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6361805578516848,\n",
       " 0.5720555192931405,\n",
       " 0.5183621193776583,\n",
       " 0.5652835688777476,\n",
       " 0.536336756118849]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/mnist.csv', header=None)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(df[0])\n",
    "X = df.drop(0, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 601 samples\n",
      "Epoch 1/1\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 14.6374 - acc: 0.0914 - val_loss: 14.4821 - val_acc: 0.1015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6462e65c0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=30, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
